{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18a0a7b9",
      "metadata": {
        "id": "18a0a7b9"
      },
      "source": [
        "\n",
        "# Семинар: классический NLP-пайплайн (spaCy) + Zipf + правила\n",
        "\n",
        "В этом ноутбуке мы:\n",
        "1) загрузим текст (Project Gutenberg: *Pride and Prejudice*);\n",
        "2) сделаем базовую предобработку и посчитаем **types/tokens**;\n",
        "3) построим **Zipf-графики** и посмотрим **длинный хвост** (≤ 3);\n",
        "4) запустим **классический NLP-пайплайн** в `spaCy` (токенизация → POS → зависимости → NER);\n",
        "5) соберём **простую систему на правилах** (rule-based intent detector) для коротких пользовательских запросов.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3ec37916",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ec37916",
        "outputId": "f77b43e5-073b-4189-d2fe-f82f711b079e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# (Colab) Установка зависимостей\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e61a34d5",
      "metadata": {
        "id": "e61a34d5"
      },
      "source": [
        "## 1) Загрузка данных (Gutenberg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "072fdce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "072fdce9",
        "outputId": "1cbd1816-56dc-45d9-a828-4f0bb72eb7dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Длина текста (символов): 237873\n",
            "Андрей Платонов.\n",
            "\n",
            "                                 Котлован\n",
            "\n",
            "                OCR: Serge Winitzki (swinitzk@hotmail.com)\n",
            "\n",
            "\n",
            "    В  день  тридцатилетия  личной  жизни  Вощеву дали расчет с\n",
            "небольшого механического завода, где он  добывал  средства  для\n",
            "своего  существования. В увольнительном документе ему написали,\n",
            "что   он   устраняется   с   производства   вследствие    роста\n",
            "слабосильности в нем и\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# import re\n",
        "# from collections import Counter\n",
        "# import requests\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# URL = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
        "\n",
        "# raw = requests.get(URL, timeout=30).text\n",
        "\n",
        "# Уберём служебные заголовки/хвосты Gutenberg (если маркеры есть)\n",
        "# start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "# end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "# start_idx = raw.find(start_marker)\n",
        "# end_idx = raw.find(end_marker)\n",
        "\n",
        "# text = raw\n",
        "# if start_idx != -1:\n",
        "#     text = raw[start_idx:]\n",
        "#     text = text.split(\"\\n\", 1)[1] if \"\\n\" in text else text\n",
        "# if end_idx != -1 and end_idx > 0:\n",
        "#     text = text[:end_idx]\n",
        "\n",
        "text = Path(\"Kotlovan.txt\").read_text(encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "print(\"Длина текста (символов):\", len(text))\n",
        "print(text[:400])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c7e70c50",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens_raw_lower tokens= 34187 types= 9808 long_tail_with_types less than 4 = 8545 (87.12%)\n",
            "top 20: [('и', 1553), ('в', 879), ('не', 657), ('на', 572), ('он', 420), ('а', 419), ('что', 343), ('с', 287), ('чиклин', 284), ('его', 246), ('я', 213), ('от', 206), ('ты', 200), ('как', 199), ('но', 181), ('все', 178), ('к', 171), ('вощев', 169), ('же', 144), ('по', 143)]\n",
            "tokens_no_puctuation tokens= 34412 types= 9747 long_tail_with_types less than 4 = 8482 (87.02%)\n",
            "top 20: [('и', 1555), ('в', 879), ('не', 657), ('на', 572), ('он', 420), ('а', 419), ('что', 379), ('с', 287), ('чиклин', 284), ('его', 246), ('я', 216), ('как', 206), ('от', 206), ('то', 204), ('ты', 200), ('но', 181), ('все', 181), ('к', 171), ('вощев', 169), ('по', 156)]\n",
            "tokens_no_puctuation_yo_to_e tokens= 34412 types= 9747 long_tail_with_types less than 4 = 8482 (87.02%)\n",
            "top 20: [('и', 1555), ('в', 879), ('не', 657), ('на', 572), ('он', 420), ('а', 419), ('что', 379), ('с', 287), ('чиклин', 284), ('его', 246), ('я', 216), ('как', 206), ('от', 206), ('то', 204), ('ты', 200), ('но', 181), ('все', 181), ('к', 171), ('вощев', 169), ('по', 156)]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize_rus(s: str):\n",
        "    return re.findall(r\"[А-Яа-яЁё]+(?:-[А-Яа-яЁё]+)*\", s)\n",
        "\n",
        "def normalize_basic(s: str, lower=True, yo_to_e=False, drop_punctuation=False):\n",
        "    if yo_to_e:\n",
        "        s = s.replace(\"ё\", \"е\").replace(\"Ё\", \"Е\")\n",
        "    if lower:\n",
        "        s = s.lower()\n",
        "    if drop_punctuation:\n",
        "        s = re.sub(r\"[^А-Яа-яЁё\\s]+\", \" \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def stats_to_tokens(tokens):\n",
        "    count = Counter(tokens)\n",
        "    N = sum(count.values())\n",
        "    V = len(count)\n",
        "    long_tail = sum(1 for f in count.values() if f <= 3)\n",
        "    return count, N, V, long_tail, long_tail / V\n",
        "\n",
        "\n",
        "versions = {\n",
        "    \"tokens_raw_lower\": normalize_basic(text, lower=True, yo_to_e=False, drop_punctuation=False),\n",
        "    \"tokens_no_puctuation\": normalize_basic(text, lower=True, yo_to_e=False, drop_punctuation=True),\n",
        "    \"tokens_no_puctuation_yo_to_e\": normalize_basic(text, lower=True, yo_to_e=True, drop_punctuation=True)\n",
        "}\n",
        "\n",
        "for version_name, version_contents in versions.items():\n",
        "    tokens = tokenize_rus(version_contents)\n",
        "    count, N, V, long_tail, long_tail_divided = stats_to_tokens(tokens)\n",
        "    print(version_name, \"tokens=\", N, \"types=\", V, \"long_tail_with_types less than 4 =\", long_tail, f\"({long_tail_divided:.2%})\")\n",
        "    print(\"top 20:\", count.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c46bb10a",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2e5bd7b6",
      "metadata": {
        "id": "2e5bd7b6"
      },
      "source": [
        "## 2) Токенизация, types/tokens, частоты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fbb210a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbb210a9",
        "outputId": "8dcb2fa1-eed9-4763-82c1-1cc426cf946f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens (всего словоупотреблений): 69\n",
            "Types  (уникальных слов):        35\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('x', 30),\n",
              " ('serge', 2),\n",
              " ('winitzki', 2),\n",
              " ('the', 2),\n",
              " ('may', 2),\n",
              " ('for', 2),\n",
              " ('ocr', 1),\n",
              " ('swinitzk', 1),\n",
              " ('hotmail', 1),\n",
              " ('com', 1)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Простая токенизация для частот: слова из латинских букв + апострофы\n",
        "tokens = re.findall(r\"[A-Za-z]+(?:'[A-Za-z]+)?\", text.lower())\n",
        "cnt = Counter(tokens)\n",
        "\n",
        "N_tokens = sum(cnt.values())  # tokens = все употребления\n",
        "V_types = len(cnt)            # types  = уникальные слова\n",
        "\n",
        "print(f\"Tokens (всего словоупотреблений): {N_tokens:,}\")\n",
        "print(f\"Types  (уникальных слов):        {V_types:,}\")\n",
        "\n",
        "# Топ-20\n",
        "top20 = cnt.most_common(20)\n",
        "top20[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff75383",
      "metadata": {
        "id": "aff75383"
      },
      "source": [
        "## 3) Zipf: rank–frequency (log–log) + топ-20 + длинный хвост ≤ 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee384fbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ee384fbc",
        "outputId": "c78cbe73-a7c0-479a-a784-f71f33d2c09b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Zipf данные\n",
        "freqs_sorted = np.array(sorted(cnt.values(), reverse=True))\n",
        "ranks = np.arange(1, len(freqs_sorted) + 1)\n",
        "\n",
        "# 3.1 Zipf log-log\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.loglog(ranks, freqs_sorted, marker=\".\", linestyle=\"none\")\n",
        "plt.title(\"Pride and Prejudice: закон Ципфа (ранг–частота)\")\n",
        "plt.xlabel(\"Ранг слова (log)\")\n",
        "plt.ylabel(\"Частота (log)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3.2 Топ-20\n",
        "df_top20 = pd.DataFrame(top20, columns=[\"word\",\"count\"])\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(df_top20[\"word\"], df_top20[\"count\"])\n",
        "plt.title(\"Топ-20 слов по частоте\")\n",
        "plt.xlabel(\"Слово\")\n",
        "plt.ylabel(\"Частота\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3.3 Длинный хвост: <= 3\n",
        "tail_words = [w for w, c in cnt.items() if c <= 3]\n",
        "tail_types = len(tail_words)\n",
        "tail_tokens = sum(cnt[w] for w in tail_words)\n",
        "\n",
        "print(f\"Хвост (c<=3): {tail_types} типов = {tail_types/V_types:.1%} словаря\")\n",
        "print(f\"Хвост (c<=3): {tail_tokens} токенов = {tail_tokens/N_tokens:.1%} текста\")\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar([\"Доля ТИПОВ\\n(c ≤ 3)\", \"Доля ТОКЕНОВ\\n(c ≤ 3)\"],\n",
        "        [tail_types/V_types, tail_tokens/N_tokens])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"«Длинный хвост» (c ≤ 3)\")\n",
        "plt.ylabel(\"Доля\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3.4 (Опционально) 10 случайных hapax (c=1)\n",
        "hapax = [w for w, c in cnt.items() if c == 1]\n",
        "rng = np.random.default_rng(0)\n",
        "sample10 = list(rng.choice(hapax, size=10, replace=False))\n",
        "sample10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b08068f",
      "metadata": {
        "id": "5b08068f"
      },
      "source": [
        "## 4) Классический NLP-пайплайн в spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92a688e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a92a688e",
        "outputId": "f4eff8b5-96be-429a-f86f-4af76dfffc41"
      },
      "outputs": [],
      "source": [
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(\"Mr. Darcy spoke politely, but Elizabeth did not change her mind about him. Then, he went to the table.\")\n",
        "print(\"SENTENCES:\", [s.text for s in doc.sents])\n",
        "print(\"TOKENS:\", [t.text for t in doc])\n",
        "print(\"POS:\", [(t.text, t.pos_) for t in doc])\n",
        "print(\"DEP:\", [(t.text, t.dep_, t.head.text) for t in doc])\n",
        "\n",
        "print(\"\\nNER entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n",
        "for t in doc:\n",
        "    print(f\"{t.i:2d}  {t.text!r:12}  is_alpha={t.is_alpha}  is_punct={t.is_punct}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "766dc03b",
      "metadata": {
        "id": "766dc03b"
      },
      "source": [
        "### Применим пайплайн к реальному фрагменту из книги"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd0188d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "bd0188d2",
        "outputId": "5e960782-71b1-4f42-e837-899b92d28657"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Возьмём небольшой фрагмент (чтобы быстро работало)\n",
        "snippet = \" \".join(text.split()[:350])  # первые ~350 слов\n",
        "doc = nlp(snippet)\n",
        "\n",
        "# Посмотрим 15 токенов с POS/леммой\n",
        "rows = []\n",
        "for t in list(doc)[:15]:\n",
        "    rows.append([t.text, t.lemma_, t.pos_, t.tag_])\n",
        "pd.DataFrame(rows, columns=[\"token\",\"lemma\",\"POS\",\"TAG\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2893dc9",
      "metadata": {
        "id": "f2893dc9"
      },
      "source": [
        "## 5) Простая rule-based система: определение интентов"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b60814f9",
      "metadata": {
        "id": "b60814f9"
      },
      "source": [
        "\n",
        "Сделаем игрушечный **intent detector** для пользовательских сообщений:\n",
        "- greeting / goodbye\n",
        "- ask_weather\n",
        "- ask_time\n",
        "- math_addition\n",
        "- other\n",
        "\n",
        "Подход:\n",
        "- регулярки + `spaCy Matcher`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bf5298",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0bf5298",
        "outputId": "abf74e98-f449-4c66-8f96-3c1c0830d5b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern_greet = [{\"LOWER\": {\"IN\": [\"hi\",\"hello\",\"hey\",\"greetings\"]}}]\n",
        "pattern_goodbye = [{\"LOWER\": {\"IN\": [\"bye\",\"goodbye\",\"farewell\"]}}]\n",
        "pattern_weather = [{\"LOWER\": {\"IN\": [\"weather\",\"rain\",\"sunny\",\"forecast\"]}}]\n",
        "pattern_time = [{\"LOWER\": {\"IN\": [\"time\",\"clock\"]}}]\n",
        "\n",
        "pattern_add_1 = [{\"LOWER\": \"add\"}, {\"IS_DIGIT\": True}, {\"LOWER\": \"to\"}, {\"IS_DIGIT\": True}]\n",
        "pattern_add_2 = [{\"IS_DIGIT\": True}, {\"TEXT\": \"+\"}, {\"IS_DIGIT\": True}]\n",
        "\n",
        "matcher.add(\"GREET\", [pattern_greet])\n",
        "matcher.add(\"GOODBYE\", [pattern_goodbye])\n",
        "matcher.add(\"WEATHER\", [pattern_weather])\n",
        "matcher.add(\"TIME\", [pattern_time])\n",
        "matcher.add(\"ADD\", [pattern_add_1, pattern_add_2])\n",
        "\n",
        "def detect_intent(text: str):\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "    labels = [nlp.vocab.strings[m_id] for m_id, _, _ in matches]\n",
        "\n",
        "    if \"ADD\" in labels:\n",
        "        nums = [int(t.text) for t in doc if t.like_num]\n",
        "        if len(nums) >= 2:\n",
        "            return \"math_addition\", nums[0] + nums[1]\n",
        "        return \"math_addition\", None\n",
        "    if \"WEATHER\" in labels:\n",
        "        return \"ask_weather\", None\n",
        "    if \"TIME\" in labels:\n",
        "        return \"ask_time\", None\n",
        "    if \"GREET\" in labels:\n",
        "        return \"greeting\", None\n",
        "    if \"GOODBYE\" in labels:\n",
        "        return \"goodbye\", None\n",
        "    return \"other\", None\n",
        "\n",
        "tests = [\n",
        "    \"Hello!\",\n",
        "    \"Add 34957 to 70764\",\n",
        "    \"What is 2 + 2 ?\",\n",
        "    \"What's the weather tomorrow?\",\n",
        "    \"What time is it now?\",\n",
        "    \"Goodbye!\",\n",
        "    \"Explain Zipf's law in one sentence.\"\n",
        "]\n",
        "\n",
        "for s in tests:\n",
        "    intent, val = detect_intent(s)\n",
        "    print(f\"{s!r} -> {intent}\", (f\"(value={val})\" if val is not None else \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1874f7",
      "metadata": {
        "id": "3a1874f7"
      },
      "source": [
        "## Домашняя мини-задача (5–10 минут)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e810668e",
      "metadata": {
        "id": "e810668e"
      },
      "source": [
        "\n",
        "1) Возьмите **русский роман** (любой .txt) и повторите блок **Zipf + длинный хвост**.\n",
        "2) Сравните предобработки: lowercasing, удаление пунктуации, замена \"ё\"→\"е\", лемматизация (если есть инструменты).\n",
        "3) Добавьте 2 новых интента и правила (Matcher/regex).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc135947",
      "metadata": {
        "id": "dc135947"
      },
      "source": [
        "# Дополнение: русский язык — лемматизация, стемминг, POS/DEP/NER\n",
        "\n",
        "В этом разделе:\n",
        "1) покажем **лемматизацию** для русского (два варианта: `spaCy` и `pymorphy3`),\n",
        "2) покажем **стемминг** (SnowballStemmer),\n",
        "3) запустим **классический NLP-пайплайн** для русского: POS → синтаксические зависимости (DEP) → NER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483d16c8",
      "metadata": {
        "id": "483d16c8"
      },
      "source": [
        "## 6) Установка русских инструментов\n",
        "\n",
        "- `ru_core_news_sm` — русская модель spaCy (токены, леммы, POS, зависимости, NER)\n",
        "- `pymorphy3` — сильный морфологический анализатор/лемматизатор для русского\n",
        "- `nltk` — стемминг (SnowballStemmer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d0fcc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7d0fcc1",
        "outputId": "ad65f954-3fb4-41fd-c445-ce7a4dc54fae"
      },
      "outputs": [],
      "source": [
        "# (Colab) Установка для русского\n",
        "!pip -q install spacy==3.7.5  nltk\n",
        "!python -m spacy download ru_core_news_sm -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a310394",
      "metadata": {
        "id": "3a310394"
      },
      "source": [
        "## 7) Пример русского текста\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6a8c6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c6a8c6d",
        "outputId": "64d0699e-ca63-4c30-f2d3-128c81a3cbaa"
      },
      "outputs": [],
      "source": [
        "ru_text = (\n",
        "    \"Вчера Анна Каренина приехала в Москву.\\n\"\n",
        "    \"Она встретилась с братом Стивой в ресторане и долго говорила о семье и поездке.\\n\"\n",
        ")\n",
        "print(ru_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b4ef811",
      "metadata": {
        "id": "0b4ef811"
      },
      "source": [
        "## 8) Лемматизация и стемминг (русский)\n",
        "\n",
        "Сравним:\n",
        "- **леммы** (нормальная форма слова) — полезно для словаря/частот;\n",
        "- **стеммы** (усечённые основы) — грубее, но быстро и просто.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e836d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28e836d1",
        "outputId": "a70281be-b3f0-4c5f-a40a-44aeebf0daea"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Базовая токенизация для русского (буквы + дефис)\n",
        "ru_tokens = re.findall(r\"[А-Яа-яЁё]+(?:-[А-Яа-яЁё]+)?\", ru_text.lower())\n",
        "print(\"Токены:\", ru_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4177d163",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4177d163",
        "outputId": "f13a9276-be0c-4043-a290-aff24fe683b6"
      },
      "outputs": [],
      "source": [
        "# 8.1 Стемминг (Snowball)\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "ru_stems = [stemmer.stem(w) for w in ru_tokens]\n",
        "print(list(zip(ru_tokens, ru_stems)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7040705b",
      "metadata": {
        "id": "7040705b"
      },
      "source": [
        "### 8.2 Лемматизация через pymorphy2 (обычно лучше для русского)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_-cXvhVetM-A",
      "metadata": {
        "id": "_-cXvhVetM-A"
      },
      "outputs": [],
      "source": [
        "!pip -q install pymorphy3 pymorphy3-dicts-ru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858a3049",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "858a3049",
        "outputId": "a04308b8-1e5d-4765-df12-dafc2c378bd6"
      },
      "outputs": [],
      "source": [
        "import pymorphy3\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "ru_lemmas_pym = [morph.parse(w)[0].normal_form for w in ru_tokens]\n",
        "print(list(zip(ru_tokens, ru_lemmas_pym)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf625e4",
      "metadata": {
        "id": "dcf625e4"
      },
      "source": [
        "### 8.3 Лемматизация через spaCy (удобно, когда вы и так используете пайплайн)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f48458",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "b1f48458",
        "outputId": "38544e58-1222-4cf0-cfc8-0bafa04d1395"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "doc_ru = nlp_ru(ru_text)\n",
        "print([(t.text, t.lemma_, t.pos_) for t in doc_ru if t.is_alpha])\n",
        "\n",
        "displacy.render(doc_ru, style=\"dep\", jupyter=True, options={\"distance\": 90})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ee0b09",
      "metadata": {
        "id": "13ee0b09"
      },
      "source": [
        "## 9) POS-теги + синтаксические зависимости (DEP) + NER для русского"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202aeccf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "202aeccf",
        "outputId": "a12e56a9-9903-46d8-e200-278f1f4231f8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 9.1 POS + морфология\n",
        "rows = []\n",
        "for t in doc_ru:\n",
        "    if t.is_space:\n",
        "        continue\n",
        "    rows.append([t.text, t.lemma_, t.pos_, t.tag_, t.morph.to_json()])\n",
        "pd.DataFrame(rows, columns=[\"token\",\"lemma\",\"POS\",\"TAG\",\"morph\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1402cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "2a1402cc",
        "outputId": "c1fbc465-30a1-4e4f-99bc-f13e439eb4f9"
      },
      "outputs": [],
      "source": [
        "# 9.2 Синтаксические зависимости (DEP): token -> head\n",
        "dep_rows = []\n",
        "for t in doc_ru:\n",
        "    if t.is_space:\n",
        "        continue\n",
        "    dep_rows.append([t.text, t.dep_, t.head.text])\n",
        "pd.DataFrame(dep_rows, columns=[\"token\",\"dep\",\"head\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4906357c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4906357c",
        "outputId": "069c3021-dd00-4f99-9638-ddca75ae5fa7"
      },
      "outputs": [],
      "source": [
        "# 9.3 NER (именованные сущности)\n",
        "[(ent.text, ent.label_) for ent in doc_ru.ents]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y05VYdI5rJ2o",
      "metadata": {
        "id": "Y05VYdI5rJ2o"
      },
      "source": [
        "## Матчеры на основе тэгов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KdZtHQZ5rShT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdZtHQZ5rShT",
        "outputId": "c9f2fa83-6f92-41d2-e30a-0b340d8b7711"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern_superl_noun = [\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: the/a\n",
        "    {\"POS\": \"ADJ\", \"TAG\": {\"IN\": [\"JJS\"]}},  # superlative adj: best, worst\n",
        "    {\"POS\": \"NOUN\"}\n",
        "]\n",
        "matcher.add(\"SUPERL_NOUN\", [pattern_superl_noun])\n",
        "\n",
        "doc = nlp(\"This is the best book and the worst idea.\")\n",
        "for mid, start, end in matcher(doc):\n",
        "    print(nlp.vocab.strings[mid], doc[start:end].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oZV4KigdrU5_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZV4KigdrU5_",
        "outputId": "2d9694db-f543-47e5-8fe6-a917589c0312"
      },
      "outputs": [],
      "source": [
        "pattern_past_verb_object = [\n",
        "    {\"POS\": \"VERB\", \"TAG\": \"VBD\"},            # past tense verb\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},                # optional determiner\n",
        "    {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}}        # object-ish noun\n",
        "]\n",
        "matcher.add(\"PAST_VERB_OBJ\", [pattern_past_verb_object])\n",
        "\n",
        "doc = nlp(\"She visited London and bought a car yesterday.\")\n",
        "for mid, start, end in matcher(doc):\n",
        "    print(nlp.vocab.strings[mid], doc[start:end].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeZls01erYEL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeZls01erYEL",
        "outputId": "c5f7d725-ac09-4600-c306-29cf44b434a1"
      },
      "outputs": [],
      "source": [
        "pattern_np = [\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},\n",
        "    {\"POS\": \"ADJ\", \"OP\": \"*\"},\n",
        "    {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
        "]\n",
        "matcher.add(\"NP_RULE\", [pattern_np])\n",
        "\n",
        "doc = nlp(\"I saw a very old house and Mr. Darcy.\")\n",
        "for mid, start, end in matcher(doc):\n",
        "    print(doc[start:end].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe-NSbzhrcft",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe-NSbzhrcft",
        "outputId": "80484fc5-8b89-43c5-c54e-359a2b23ecb4"
      },
      "outputs": [],
      "source": [
        "pattern_question = [\n",
        "    {\"POS\": \"AUX\"},\n",
        "    {\"POS\": \"PRON\"},\n",
        "    {\"POS\": \"VERB\"}\n",
        "]\n",
        "matcher.add(\"QUESTION_AUX_PRON_VERB\", [pattern_question])\n",
        "\n",
        "doc = nlp(\"Do you know Zipf's law? Can you help me?\")\n",
        "for mid, start, end in matcher(doc):\n",
        "    print(nlp.vocab.strings[mid], doc[start:end].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J3zsBvs5rh0J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3zsBvs5rh0J",
        "outputId": "186eeaa7-4dc9-49d6-9ed6-33c5dccab77a"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp_ru.vocab)\n",
        "\n",
        "pattern_adj_noun = [\n",
        "    {\"POS\": \"ADJ\"},\n",
        "    {\"POS\": \"NOUN\"}\n",
        "]\n",
        "matcher.add(\"ADJ_NOUN\", [pattern_adj_noun])\n",
        "\n",
        "doc = nlp_ru(\"Старый дом стоял на тихой улице.\")\n",
        "for mid, start, end in matcher(doc):\n",
        "    print(nlp_ru.vocab.strings[mid], doc[start:end].text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9800bea9",
      "metadata": {
        "id": "9800bea9"
      },
      "source": [
        "## 10) Мини-задача для семинара (русский)\n",
        "\n",
        "1) Возьмите фрагмент русского романа (например, 30–100k символов).\n",
        "2) Сравните частоты **по токенам**, **по стеммам**, **по леммам**:\n",
        "   - Как меняются топ-20?\n",
        "   - Как меняется доля “длинного хвоста” (≤3) по **types**?\n",
        "3) Выберите 5 предложений и сравните:\n",
        "   - POS/DEP/NER от `spaCy`\n",
        "   - леммы от `pymorphy3`\n",
        "4) Составьте список имен персонажей\n",
        "5) Найдите как можно больше предложений, где здороваются\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289f0684",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
